{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e78c0d97-d670-4262-9c6b-56abaf2eeb6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!mkdir squad\n",
    "!wget https://raw.githubusercontent.com/chiahsuan156/Spoken-SQuAD/master/spoken_train-v1.1.json  -O squad/train-v2.0.json\n",
    "!wget https://raw.githubusercontent.com/chiahsuan156/Spoken-SQuAD/master/spoken_test-v1.1_WER54.json -O squad/dev-v2.0.json\n",
    "!pip install transformers evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "496c8e61-278b-487a-99b4-cd4fb2632353",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, AdamW\n",
    "import time\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37e90fe8-353c-4db4-8af8-90d42b51d303",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Examples: 37111 Validation Examples: 17841\n",
      "Sample Random Train Example:\n",
      "Text: the final showdown with between justin carini one of the early favorites and kelly clarkson. clarkson was not initially thought of as a contender but impress the judges with some good performances in the final rounds such as her performance ever read the franklin natural woman and benny heightens stuff like that there and eventually won the crown on september fourth ten thousand two.\n",
      "Query: What month did Kelly Clarkson win?\n",
      "Answer: {'answer_start': 352, 'text': 'september'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "# Load data from JSON files\n",
    "def load_squad_data(path):\n",
    "    with open(path, 'r') as f:\n",
    "        squad_dict = json.load(f)\n",
    "    texts, queries, answers = [], [], []\n",
    "    for group in squad_dict['data']:\n",
    "        for passage in group['paragraphs']:\n",
    "            context = passage['context']\n",
    "            for qa in passage['qas']:\n",
    "                question = qa['question']\n",
    "                # Check if the question is answerable (use default False if 'is_impossible' is not present)\n",
    "                if qa.get('is_impossible', False):\n",
    "                    continue  # Skip unanswerable questions for simplicity\n",
    "                for answer in qa['answers']:\n",
    "                    texts.append(context)\n",
    "                    queries.append(question)\n",
    "                    answers.append({\n",
    "                        'answer_start': answer['answer_start'],\n",
    "                        'text': answer['text']\n",
    "                    })\n",
    "    return texts, queries, answers\n",
    "\n",
    "# Specify the paths for the SQuAD data files\n",
    "train_path = Path('squad/train-v2.0.json')\n",
    "val_path = Path('squad/dev-v2.0.json')\n",
    "\n",
    "# Load train and validation data\n",
    "train_texts, train_queries, train_answers = load_squad_data(train_path)\n",
    "val_texts, val_queries, val_answers = load_squad_data(val_path)\n",
    "\n",
    "# Display a random example from the training data\n",
    "random_index = random.randint(0, len(train_texts) - 1)\n",
    "print(\"Train Examples:\", len(train_texts), \"Validation Examples:\", len(val_texts))\n",
    "print(\"Sample Random Train Example:\")\n",
    "print(\"Text:\", train_texts[random_index])\n",
    "print(\"Query:\", train_queries[random_index])\n",
    "print(\"Answer:\", train_answers[random_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8589e151-b3ff-4017-9817-d295938b671a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ensure each answer has an 'answer_end'\n",
    "def set_end_positions(answers):\n",
    "    for answer in answers:\n",
    "        start_idx = answer['answer_start']\n",
    "        answer['answer_end'] = start_idx + len(answer['text'])\n",
    "\n",
    "set_end_positions(train_answers)\n",
    "set_end_positions(val_answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36006a76-67ff-4546-b622-ab512dc50526",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"  # or another suitable QA model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Now tokenize each example with `encode_plus`\n",
    "train_encodings = {\n",
    "    'input_ids': [],\n",
    "    'attention_mask': [],\n",
    "    'start_positions': [],\n",
    "    'end_positions': []\n",
    "}\n",
    "\n",
    "for text, query in zip(train_texts, train_queries):\n",
    "    encodings = tokenizer.encode_plus(\n",
    "        text,\n",
    "        query,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        stride=128,\n",
    "        return_tensors=\"pt\",\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "    train_encodings['input_ids'].append(encodings['input_ids'])\n",
    "    train_encodings['attention_mask'].append(encodings['attention_mask'])\n",
    "\n",
    "# Similar approach for validation data\n",
    "val_encodings = {\n",
    "    'input_ids': [],\n",
    "    'attention_mask': []\n",
    "}\n",
    "\n",
    "for text, query in zip(val_texts, val_queries):\n",
    "    encodings = tokenizer.encode_plus(\n",
    "        text,\n",
    "        query,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        stride=128,\n",
    "        return_tensors=\"pt\",\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "    val_encodings['input_ids'].append(encodings['input_ids'])\n",
    "    val_encodings['attention_mask'].append(encodings['attention_mask'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7d7bd51-fd9e-4d70-9421-f2a9b6b374d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adjust the function to add start and end positions for each encoding\n",
    "def add_token_positions(texts, queries, answers):\n",
    "    encodings = {\n",
    "        'input_ids': [],\n",
    "        'attention_mask': [],\n",
    "        'start_positions': [],\n",
    "        'end_positions': []\n",
    "    }\n",
    "\n",
    "    for text, query, answer in zip(texts, queries, answers):\n",
    "        # Encode text and query with overflow and stride\n",
    "        encoding = tokenizer(\n",
    "            text,\n",
    "            query,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512,\n",
    "            stride=128,\n",
    "            return_tensors=\"pt\",\n",
    "            return_offsets_mapping=True,\n",
    "            return_overflowing_tokens=True\n",
    "        )\n",
    "\n",
    "        # Find start and end positions within each encoding\n",
    "        offset_mapping = encoding['offset_mapping'][0]\n",
    "        start_char = answer['answer_start']\n",
    "        end_char = answer['answer_end']\n",
    "        \n",
    "        # Initialize start and end token positions as None\n",
    "        start_token = None\n",
    "        end_token = None\n",
    "\n",
    "        # Loop through offset mapping to find the token positions\n",
    "        for idx, (start, end) in enumerate(offset_mapping):\n",
    "            if start <= start_char < end:\n",
    "                start_token = idx\n",
    "            if start < end_char <= end:\n",
    "                end_token = idx\n",
    "                break\n",
    "\n",
    "        # Handle cases where token positions are not found\n",
    "        if start_token is None:\n",
    "            start_token = tokenizer.model_max_length\n",
    "        if end_token is None:\n",
    "            end_token = tokenizer.model_max_length\n",
    "\n",
    "        # Append the results for each instance\n",
    "        encodings['input_ids'].append(encoding['input_ids'][0])\n",
    "        encodings['attention_mask'].append(encoding['attention_mask'][0])\n",
    "        encodings['start_positions'].append(start_token)\n",
    "        encodings['end_positions'].append(end_token)\n",
    "\n",
    "    # Convert lists to tensors for compatibility\n",
    "    encodings['input_ids'] = torch.stack(encodings['input_ids'])\n",
    "    encodings['attention_mask'] = torch.stack(encodings['attention_mask'])\n",
    "    encodings['start_positions'] = torch.tensor(encodings['start_positions'])\n",
    "    encodings['end_positions'] = torch.tensor(encodings['end_positions'])\n",
    "\n",
    "    return encodings\n",
    "\n",
    "# Apply the function to create encodings with token positions\n",
    "train_encodings = add_token_positions(train_texts, train_queries, train_answers)\n",
    "val_encodings = add_token_positions(val_texts, val_queries, val_answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a0e31c7-7d59-4a7e-9e8f-2ce4c9151ee0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SquadDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "train_dataset = SquadDataset(train_encodings)\n",
    "val_dataset = SquadDataset(val_encodings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a32691c-0364-4085-9c20-cc062551aa2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bb13d24-a68e-475e-aecb-f4c39f486b7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\").to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ab5314e-e15b-4ad5-84c0-2147b570be6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_scratch/slurm.997101/ipykernel_104046/2853054750.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000/9278, Loss: 2.0218353271484375\n",
      "Step 2000/9278, Loss: 1.7225341796875\n",
      "Step 3000/9278, Loss: 1.4822540283203125\n",
      "Step 4000/9278, Loss: 1.4004745483398438\n",
      "Step 5000/9278, Loss: 1.221822738647461\n",
      "Step 6000/9278, Loss: 0.5452346801757812\n",
      "Step 7000/9278, Loss: 2.1209716796875\n",
      "Step 8000/9278, Loss: 1.0061416625976562\n",
      "Step 9000/9278, Loss: 1.23480224609375\n",
      "Epoch 1/5\n",
      "Training Loss: 1.5023252016050237, Validation Loss: nan\n",
      "Step 1000/9278, Loss: 1.667999267578125\n",
      "Step 2000/9278, Loss: 2.9754638671875\n",
      "Step 3000/9278, Loss: 1.7955322265625\n",
      "Step 4000/9278, Loss: 1.4657363891601562\n",
      "Step 5000/9278, Loss: 1.257904052734375\n",
      "Step 6000/9278, Loss: 2.203155517578125\n",
      "Step 7000/9278, Loss: 1.1234588623046875\n",
      "Step 8000/9278, Loss: 0.2133312225341797\n",
      "Step 9000/9278, Loss: 1.7354736328125\n",
      "Epoch 2/5\n",
      "Training Loss: 1.271622144317005, Validation Loss: nan\n",
      "Step 1000/9278, Loss: 0.5979766845703125\n",
      "Step 2000/9278, Loss: 0.7570538520812988\n",
      "Step 3000/9278, Loss: 0.5679855346679688\n",
      "Step 4000/9278, Loss: 0.5498495101928711\n",
      "Step 5000/9278, Loss: 0.7583045959472656\n",
      "Step 6000/9278, Loss: 0.7403755187988281\n",
      "Step 7000/9278, Loss: 2.8050079345703125\n",
      "Step 8000/9278, Loss: 0.5709705352783203\n",
      "Step 9000/9278, Loss: 1.4242706298828125\n",
      "Epoch 3/5\n",
      "Training Loss: 1.0954992139752529, Validation Loss: nan\n",
      "Step 1000/9278, Loss: 0.20693540573120117\n",
      "Step 2000/9278, Loss: 1.7983245849609375\n",
      "Step 3000/9278, Loss: 1.912576675415039\n",
      "Step 4000/9278, Loss: 1.3083374500274658\n",
      "Step 5000/9278, Loss: 1.0381903648376465\n",
      "Step 6000/9278, Loss: 0.163069486618042\n",
      "Step 7000/9278, Loss: 0.8128633499145508\n",
      "Step 8000/9278, Loss: 0.691131591796875\n",
      "Step 9000/9278, Loss: 0.7195358276367188\n",
      "Epoch 4/5\n",
      "Training Loss: 0.9783861052295493, Validation Loss: nan\n",
      "Step 1000/9278, Loss: 2.4624290466308594\n",
      "Step 2000/9278, Loss: 0.607757568359375\n",
      "Step 3000/9278, Loss: 0.8988161087036133\n",
      "Step 4000/9278, Loss: 0.9974727630615234\n",
      "Step 5000/9278, Loss: 1.078322410583496\n",
      "Step 6000/9278, Loss: 0.6220570802688599\n",
      "Step 7000/9278, Loss: 0.6338319778442383\n",
      "Step 8000/9278, Loss: 1.0996131896972656\n",
      "Step 9000/9278, Loss: 1.2873001098632812\n",
      "Epoch 5/5\n",
      "Training Loss: 0.9015654139164502, Validation Loss: nan\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.amp import autocast, GradScaler  # Updated import for amp\n",
    "\n",
    "epochs = 5\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "print_every = 1000\n",
    "scaler = GradScaler(\"cuda\")  # Updated GradScaler usage\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Training Loop\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Use mixed precision with autocast\n",
    "        with autocast(\"cuda\"):  # Updated autocast usage\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            start_positions = batch['start_positions'].to(device)\n",
    "            end_positions = batch['end_positions'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
    "            loss = outputs.loss\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        # Scale loss for mixed precision\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Print loss every `print_every` steps\n",
    "        if (step + 1) % print_every == 0:\n",
    "            print(f\"Step {step + 1}/{len(train_loader)}, Loss: {loss.item()}\")\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation (Evaluated every epoch in this case, can be adjusted)\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    val_preds, val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            start_positions = batch['start_positions'].to(device)\n",
    "            end_positions = batch['end_positions'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
    "            loss = outputs.loss\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            start_logits, end_logits = outputs.start_logits, outputs.end_logits\n",
    "\n",
    "            # Collect token indices instead of decoding to text for each step\n",
    "            for i in range(len(input_ids)):\n",
    "                start_idx = torch.argmax(start_logits[i])\n",
    "                end_idx = torch.argmax(end_logits[i]) + 1\n",
    "                val_preds.append((input_ids[i][start_idx:end_idx]).tolist())\n",
    "                val_labels.append((input_ids[i][batch['start_positions'][i]:batch['end_positions'][i] + 1]).tolist())\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    # Print training and validation loss\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    print(f\"Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45dfcc03-2abb-484d-a25d-4f1a7ce88f4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_f1(pred, true):\n",
    "    pred_tokens = pred.split()\n",
    "    true_tokens = true.split()\n",
    "    common = set(pred_tokens) & set(true_tokens)\n",
    "    \n",
    "    if len(common) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    precision = len(common) / len(pred_tokens)\n",
    "    recall = len(common) / len(true_tokens)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f9e39ae-f6c1-4c44-b388-a7099fdb201e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def compute_f1(predictions, labels):\n",
    "    def f1_score(pred, label):\n",
    "        pred_tokens = Counter(pred.split())\n",
    "        label_tokens = Counter(label.split())\n",
    "        \n",
    "        # Find common tokens between prediction and label\n",
    "        common_tokens = pred_tokens & label_tokens\n",
    "        num_common = sum(common_tokens.values())\n",
    "        \n",
    "        if num_common == 0:\n",
    "            return 0.0\n",
    "\n",
    "        precision = num_common / sum(pred_tokens.values())\n",
    "        recall = num_common / sum(label_tokens.values())\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        return f1\n",
    "\n",
    "    # Calculate F1 for each prediction-label pair and average them\n",
    "    f1_scores = [f1_score(pred, label) for pred, label in zip(predictions, labels)]\n",
    "    return sum(f1_scores) / len(f1_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29a7293a-7d92-41f7-ba1f-0ae745f7c2c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_predictions(model, tokenizer, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions, labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            start_positions = batch['start_positions'].to(device)\n",
    "            end_positions = batch['end_positions'].to(device)\n",
    "\n",
    "            # Forward pass to get logits\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            start_logits, end_logits = outputs.start_logits, outputs.end_logits\n",
    "\n",
    "            # Find the best start and end positions for the answer\n",
    "            for i in range(len(input_ids)):\n",
    "                start_idx = torch.argmax(start_logits[i])\n",
    "                end_idx = torch.argmax(end_logits[i]) + 1\n",
    "\n",
    "                # Decode predicted answer and true answer\n",
    "                pred_text = tokenizer.decode(input_ids[i][start_idx:end_idx], skip_special_tokens=True)\n",
    "                true_text = tokenizer.decode(input_ids[i][start_positions[i]:end_positions[i] + 1], skip_special_tokens=True)\n",
    "\n",
    "                predictions.append(pred_text)\n",
    "                labels.append(true_text)\n",
    "\n",
    "    return predictions, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da988d3-61bc-4c14-bbb6-cae9950dd48c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_scratch/slurm.997101/ipykernel_104046/2853054750.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    }
   ],
   "source": [
    "# Assuming `val_loader` is your validation DataLoader and `device` is set to \"cuda\" or \"cpu\"\n",
    "predictions, labels = get_predictions(model, tokenizer, val_loader, device)\n",
    "f1 = compute_f1(predictions, labels)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36bd1614-7b0f-4ebb-a7ba-b64c23029189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f71378f407aa4b63ac228200722802e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fca8ff2fcd9544c09ebe6f68b08545c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a3cc8ec23c474a80b56c7aac547296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4016e2757a8f4b6c946d05832d88306d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0f58364e7b9479f88183a22a3d9c00d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f1\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Evaluate the pretrained model on your test loader\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m f1_score \u001b[38;5;241m=\u001b[39m evaluate_model(model, tokenizer, test_loader, device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_loader' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "# Load pretrained QA model and tokenizer\n",
    "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Evaluation function as previously defined\n",
    "def evaluate_model(model, tokenizer, test_loader, device):\n",
    "    model.eval()\n",
    "    predictions, labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            start_positions = batch['start_positions'].to(device)\n",
    "            end_positions = batch['end_positions'].to(device)\n",
    "\n",
    "            # Forward pass to get logits\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            start_logits, end_logits = outputs.start_logits, outputs.end_logits\n",
    "\n",
    "            for i in range(len(input_ids)):\n",
    "                # Get the predicted start and end indices\n",
    "                start_idx = torch.argmax(start_logits[i])\n",
    "                end_idx = torch.argmax(end_logits[i]) + 1\n",
    "\n",
    "                # Decode predicted answer and true answer\n",
    "                pred_text = tokenizer.decode(input_ids[i][start_idx:end_idx], skip_special_tokens=True)\n",
    "                true_text = tokenizer.decode(input_ids[i][batch['start_positions'][i]:batch['end_positions'][i]+1], skip_special_tokens=True)\n",
    "\n",
    "                predictions.append(pred_text)\n",
    "                labels.append(true_text)\n",
    "\n",
    "    f1 = compute_f1(predictions, labels)\n",
    "    print(\"F1 Score on test set:\", f1)\n",
    "    return f1\n",
    "\n",
    "# Evaluate the pretrained model on your test loader\n",
    "f1_score = evaluate_model(model, tokenizer, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec489856-80ed-4223-b0b1-bef1cee79ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
